{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333acb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import train\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random,yaml\n",
    "import functools\n",
    "import pathlib,os,sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7620591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and weights...\n"
     ]
    }
   ],
   "source": [
    "# To make all paths relative\n",
    "base_path = pathlib.Path().absolute()\n",
    "\n",
    "# Importing configurations\n",
    "yml_path = f\"{base_path}/config/config.yml\"\n",
    "with open(yml_path, \"r\") as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)\n",
    "model_name = cfg[\"params\"][\"model_name\"]\n",
    "\n",
    "# Loading model and pretrained weights\n",
    "print(\"Loading model and weights...\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "checkpoint = torch.load(f\"{base_path}/weights/best_model11.pth\")\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "model.to('cpu')\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "051725a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    Function: converts words to tokens.\n",
    "    Input: Word\n",
    "    Output: tokens, attention-mask\n",
    "    '''\n",
    "    res = tokenizer.encode_plus(text, padding=\"max_length\")\n",
    "    return torch.tensor(res.input_ids), torch.tensor(res.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bae378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, model):\n",
    "    '''\n",
    "    Function: Prediction\n",
    "    Input: sentence, model\n",
    "    Output: NIL\n",
    "    '''\n",
    "    inp_ids, inp_mask = tokenize(sentence)\n",
    "    inp_ids = inp_ids.unsqueeze(0)\n",
    "    inp_ids = inp_ids.to('cpu')\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids = inp_ids)\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce8b90f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Great taste\n"
     ]
    }
   ],
   "source": [
    "# Type the sentence to be processed.\n",
    "sentence = \"The taste of the cake was fantastic. It just lacked some cherry on the top..\"\n",
    "predict(sentence, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6124be0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3.7.15 ('sentiment')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "38a168c894b551566bc22ed39ff8d30089d47be378d336dc893f339e4395de14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
